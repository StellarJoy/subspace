#!/usr/bin/env python
# Copyright (c) 2024, NVIDIA CORPORATION.  All rights reserved.
#
# NVIDIA CORPORATION and its licensors retain all intellectual property
# and proprietary rights in and to this software, related documentation
# and any modifications thereto.  Any use, reproduction, disclosure or
# distribution of this software and related documentation without an express
# license agreement from NVIDIA CORPORATION is strictly prohibited.

import os
import sys
import json
import time
import fire
import torch
import transformers
from datasets import load_dataset
from peft import (
    LoraConfig,
    get_peft_model,
    prepare_model_for_kbit_training,
    set_peft_model_state_dict,
)
from transformers import AutoModelForCausalLM, AutoTokenizer, LlamaTokenizer, TrainerCallback

from stella import StellaConfig, StellaTrainer

from utils.unified_logger import UnifiedLoggerCallback

def train(
    # model/data params
    base_model: str = '',  # the only required argument
    data_path: str = 'yahma/alpaca-cleaned',
    output_dir: str = './lora-alpaca',
    adapter_name: str = 'lora',
    # training hyperparams
    batch_size: int = 128,
    micro_batch_size: int = 4,
    num_epochs: int = 3,
    optimizer: str = 'adamw',
    sgd_momentum: float = 0.9,
    learning_rate: float = 3e-4,
    weight_decay: float = 0.0,
    lr_scheduler_type: str = 'linear',
    cutoff_len: int = 256,
    val_set_size: int = 2000,
    use_gradient_checkpointing: bool = False,
    eval_step: int = 200,
    save_step: int = 200,
    max_train_samples: int = -1,
    # lora hyperparams
    bf16: bool = True,
    fp16: bool = False,
    lora_r: int = 8,
    lora_alpha: int = 16,
    lora_dropout: float = 0.05,
    lora_target_modules: list[str] = None,
    stella_retraction: str = 'euclidean',
    stella_init: str = 'random_qr',
    stella_diag_s: bool = False,
    stella_grad_scaling: bool = True,
    non_linearity: str = 'tanh',
    adapter_dropout: float = 0.0,
    use_parallel_adapter: bool = False,
    use_adapterp: bool = False,
    target_modules: list[str] = None,
    merge: bool = False,
    # llm hyperparams
    train_on_inputs: bool = False,  # if False, masks out inputs in loss
    group_by_length: bool = False,  # faster, but produces an odd training loss curve
    resume_from_checkpoint: str = None,  # either training checkpoint or final adapter
):
    print(
        f"Finetuning model with params:\n"
        f"base_model: {base_model}\n"
        f"data_path: {data_path}\n"
        f"output_dir: {output_dir}\n"
        f"batch_size: {batch_size}\n"
        f"micro_batch_size: {micro_batch_size}\n"
        f"num_epochs: {num_epochs}\n"
        f"optimizer: {optimizer}\n"
        f"sgd_momentum: {sgd_momentum}\n"
        f"learning_rate: {learning_rate}\n"
        f"weight_decay: {weight_decay}\n"
        f"lr_scheduler_type: {lr_scheduler_type}\n"
        f"cutoff_len: {cutoff_len}\n"
        f"val_set_size: {val_set_size}\n"
        f"use_gradient_checkpointing: {use_gradient_checkpointing}\n"
        f"lora_r: {lora_r}\n"
        f"lora_alpha: {lora_alpha}\n"
        f"lora_dropout: {lora_dropout}\n"
        f"lora_target_modules: {lora_target_modules}\n"
        f"stella_retraction: {stella_retraction}\n"
        f"stella_init: {stella_init}\n"
        f"stella_diag_s: {stella_diag_s}\n"
        f"stella_grad_scaling: {stella_grad_scaling}\n"
        f"non_linearity: {non_linearity}\n"
        f"adapter_dropout: {adapter_dropout}\n"
        f"use_parallel_adapter: {use_parallel_adapter}\n"
        f"use_adapterp: {use_adapterp}\n"
        f"train_on_inputs: {train_on_inputs}\n"
        f"adapter_name: {adapter_name}\n"
        f"target_modules: {target_modules}\n"
        f"group_by_length: {group_by_length}\n"
        f"resume_from_checkpoint: {resume_from_checkpoint}\n"
        f"merge: {merge}\n"
        f"bf16: {bf16}\n"
        f"fp16: {fp16}\n"
    )
    os.environ['TOKENIZERS_PARALLELISM'] = 'false'
    torch.cuda.empty_cache()

    assert base_model, "Please specify a --base_model, e.g. --base_model='decapoda-research/llama-7b-hf'"
    gradient_accumulation_steps = batch_size // micro_batch_size

    device_map = 'auto'
    world_size = int(os.environ.get('WORLD_SIZE', 1))
    ddp = world_size != 1
    if ddp:
        device_map = {'': int(os.environ.get('LOCAL_RANK') or 0)}
        gradient_accumulation_steps = gradient_accumulation_steps // world_size

    model = AutoModelForCausalLM.from_pretrained(
        base_model,
        torch_dtype=torch.bfloat16 if bf16 else torch.float16,
        device_map=device_map,
        trust_remote_code=True,
    )

    if model.config.model_type == 'llama':
        # Due to the name of transformers' LlamaTokenizer, we have to do this
        # need to handle llama 3 separately
        if 'Llama-3' in base_model:
            print('load llama-3 tokenizer')
            tokenizer = AutoTokenizer.from_pretrained(base_model)
        else:
            tokenizer = LlamaTokenizer.from_pretrained(base_model)
    else:
        tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)

    tokenizer.pad_token_id = 0  # unk. we want this to be different from the eos token
    tokenizer.padding_side = 'left'  # Allow batched inference

    def tokenize(prompt, add_eos_token=True):
        # there's probably a way to do this with the tokenizer settings
        # but again, gotta move fast
        result = tokenizer(
            prompt,
            truncation=True,
            max_length=cutoff_len,
            padding=False,
            return_tensors=None,
        )
        if (
            result['input_ids'][-1] != tokenizer.eos_token_id
            and len(result['input_ids']) < cutoff_len
            and add_eos_token
        ):
            result['input_ids'].append(tokenizer.eos_token_id)
            if 'chatglm' not in base_model:
                result['attention_mask'].append(1)

        result['labels'] = result['input_ids'].copy()

        if 'chatglm' in base_model:
            return {'input_ids': result['input_ids'], 'labels': result['labels']}
        else:
            return result

    def generate_and_tokenize_prompt(data_point):
        # 1. 生成完整的 Prompt (包含问题 + 答案)
        full_prompt = generate_prompt(data_point)
        tokenized_full_prompt = tokenize(full_prompt)

        # 2. 如果开启了“不训练输入”（只训练回答），则进入 Mask 逻辑
        if not train_on_inputs:
            # === 关键修改：创建一个副本，彻底清空答案 ===
            user_data_point = data_point.copy()
            
            # 【核心修复点】同时清空两种格式的答案字段
            user_data_point["output"] = ""     # 针对 Alpaca/Standard 格式
            user_data_point["response"] = ""   # 针对 MetaMathQA 格式 <--- 必须加这一行！
            
            # 再次生成，此时得到的才是真正的“纯问题”Prompt
            user_prompt = generate_prompt(user_data_point)
            # ==========================================

            # 计算纯问题的长度
            tokenized_user_prompt = tokenize(user_prompt, add_eos_token=False)
            user_prompt_len = len(tokenized_user_prompt["input_ids"])

            # 执行 Mask：把纯问题部分的 Label 设为 -100
            tokenized_full_prompt["labels"] = [
                -100
            ] * user_prompt_len + tokenized_full_prompt["labels"][
                user_prompt_len:
            ]

        return tokenized_full_prompt

    # model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=use_gradient_checkpointing)
    if adapter_name.lower() == 'lora':
        config = LoraConfig(
            r=lora_r,
            lora_alpha=lora_alpha,
            target_modules=target_modules,
            lora_dropout=lora_dropout,
            init_lora_weights=stella_init,
            bias='none',
            task_type='CAUSAL_LM',
        )
    elif adapter_name.lower() == 'rslora':
        config = LoraConfig(
            r=lora_r,
            lora_alpha=lora_alpha,
            target_modules=target_modules,
            lora_dropout=lora_dropout,
            bias='none',
            task_type='CAUSAL_LM',
            use_rslora=True,
        )
    elif adapter_name.lower() == 'dora':
        config = LoraConfig(
            r=lora_r,
            lora_alpha=lora_alpha,
            target_modules=target_modules,
            lora_dropout=lora_dropout,
            bias='none',
            task_type='CAUSAL_LM',
            use_dora=True,
        )
    elif adapter_name.lower() == 'stella':
        config = StellaConfig(
            r=lora_r,
            lora_alpha=lora_alpha,
            target_modules=target_modules,
            lora_dropout=lora_dropout,
            bias='none',
            stella_retraction=stella_retraction,
            init_lora_weights=stella_init,
            stella_diag_s=stella_diag_s,
            stella_grad_scaling=stella_grad_scaling,
            task_type='CAUSAL_LM',
        )
    else:
        raise NotImplementedError
    model = get_peft_model(model, config)
    print(model)

    if data_path.endswith('.json'):
        data = load_dataset('json', data_files=data_path)
    else:
        data = load_dataset(data_path)

    if resume_from_checkpoint:
        # Check the available weights and load them
        checkpoint_name = os.path.join(resume_from_checkpoint, 'pytorch_model.bin')  # Full checkpoint
        if not os.path.exists(checkpoint_name):
            checkpoint_name = os.path.join(
                resume_from_checkpoint, 'adapter_model.bin'
            )  # only LoRA model - LoRA config above has to fit
            resume_from_checkpoint = False  # So the trainer won't try loading its state
        # The two files above have a different name depending on how they were saved, but are actually the same.
        if os.path.exists(checkpoint_name):
            print(f"Restarting from {checkpoint_name}")
            adapters_weights = torch.load(checkpoint_name)
            set_peft_model_state_dict(model, adapters_weights)
        else:
            print(f"Checkpoint {checkpoint_name} not found")

    model.print_trainable_parameters()  # Be more transparent about the % of trainable params.

    raw_dataset = data['train'].shuffle(seed=42)

    # 2. 【先切片】只取前 10000 条 (Max Samples)
    # 这样能保证你总共只用了 10k 数据，和 LoRA 一致
    if max_train_samples > 0:
        print(f"✂️  Slicing dataset to {max_train_samples} samples...")
        num_samples = min(len(raw_dataset), max_train_samples)
        raw_dataset = raw_dataset.select(range(num_samples))

    # 3. 【后划分】9:1 分割 (Train / Val)
    # 这里的 val_set_size 如果是 1000，那就是 9000 训练，1000 验证
    if val_set_size > 0:
        split_dataset = raw_dataset.train_test_split(test_size=val_set_size, shuffle=True, seed=42)
        train_data = split_dataset['train'].map(generate_and_tokenize_prompt)
        val_data = split_dataset['test'].map(generate_and_tokenize_prompt)
    else:
        train_data = raw_dataset.map(generate_and_tokenize_prompt)
        val_data = None

    if not ddp and torch.cuda.device_count() > 1:
        # keeps Trainer from trying its own DataParallelism when more than 1 gpu is available
        model.is_parallelizable = True
        model.model_parallel = True

    trainer_cls = transformers.Trainer if adapter_name.lower() != 'stella' else StellaTrainer

    unified_log_path = os.path.join(output_dir, "unified_log.jsonl")

    trainer = trainer_cls(
        model=model,
        train_dataset=train_data,
        eval_dataset=val_data,
        args=transformers.TrainingArguments(
            per_device_train_batch_size=micro_batch_size,
            gradient_accumulation_steps=gradient_accumulation_steps,
            warmup_steps=500,
            num_train_epochs=num_epochs,
            learning_rate=learning_rate,
            weight_decay=weight_decay,
            lr_scheduler_type=lr_scheduler_type,
            bf16=bf16,
            fp16=fp16,
            logging_steps=1,
            optim='adamw_torch' if optimizer == 'adamw' else optimizer,
            eval_strategy='steps' if val_set_size > 0 else 'no',
            save_strategy='steps',
            eval_steps=eval_step if val_set_size > 0 else None,
            save_steps=save_step,
            output_dir=output_dir,
            save_total_limit=3,
            load_best_model_at_end=True if val_set_size > 0 else False,
            ddp_find_unused_parameters=False if ddp else None,
            group_by_length=group_by_length,
            save_safetensors=False,
        ),
        optimizer_cls_and_kwargs=(torch.optim.SGD, dict(
            lr=learning_rate,
            weight_decay=weight_decay,
            momentum=sgd_momentum
        )) if optimizer == 'sgd' else None,
        callbacks=[UnifiedLoggerCallback(unified_log_path, "Stella")],
        data_collator=transformers.DataCollatorForSeq2Seq(
            tokenizer, pad_to_multiple_of=8, return_tensors='pt', padding=True
        ),
    )
    model.config.use_cache = False

    if torch.__version__ >= '2' and sys.platform != 'win32':
        model = torch.compile(model)

    trainer.train(resume_from_checkpoint=resume_from_checkpoint)

    if merge:
        model = model.merge_and_unload()
        tokenizer.save_pretrained(output_dir)
    model.save_pretrained(output_dir)

    print("\n If there's a warning about missing keys above, please disregard :)")


def generate_prompt(data_point):
    # === 1. 数据格式适配区域 ===
    # 情况 A: MetaMathQA 格式 (query, response)
    if 'query' in data_point and 'response' in data_point:
        instruction = data_point['query']
        input_text = ""  # 数学题通常没有额外的 input 上下文
        output = data_point['response']
    
    # 情况 B: 标准 Alpaca 格式 (instruction, input, output)
    else:
        # 使用 .get() 防止报错，如果没有则给空字符串
        instruction = data_point.get('instruction', '')
        input_text = data_point.get('input', '')
        output = data_point.get('output', '')

    # === 2. 构造 Prompt 模板 ===
    # Llama 3 官方通常建议用特殊的 Chat 模板，但这里为了兼容代码，
    # 我们继续使用 Alpaca 风格的 Prompt，效果也是很好的。
    
    if input_text:
        return f"""Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.

### Instruction:
{instruction}

### Input:
{input_text}

### Response:
{output}
"""
    else:
        return f"""Below is an instruction that describes a task. Write a response that appropriately completes the request.

### Instruction:
{instruction}

### Response:
{output}
"""


if __name__ == '__main__':
    fire.Fire(train)
