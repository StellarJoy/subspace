#!/bin/bash

# ==================== 1. é­”æ³•å‰ç¼€ (è‡ªåŠ¨å®šä½å·¥ç¨‹ç›®å½•) ====================
# è·å–å½“å‰è„šæœ¬æ‰€åœ¨çš„ç›®å½• (å³ subspace/scripts)
SCRIPT_DIR=$(cd "$(dirname "$0")" && pwd)

# è·å–é¡¹ç›®æ ¹ç›®å½• (å³ subspace/)
PROJECT_ROOT=$(dirname "$SCRIPT_DIR")

# "ç¬ç§»"åˆ°æ ¹ç›®å½•æ‰§è¡Œï¼Œç¡®ä¿æ‰€æœ‰ç›¸å¯¹è·¯å¾„éƒ½ä» subspace/ å¼€å§‹
cd "$PROJECT_ROOT"

# å°†æ ¹ç›®å½•åŠ å…¥ Python æœç´¢è·¯å¾„ï¼Œç¡®ä¿èƒ½ import Fira ç­‰æ¨¡å—
export PYTHONPATH="$PROJECT_ROOT:$PYTHONPATH"

echo "ğŸ“ å½“å‰å·¥ä½œç›®å½•: $(pwd)"

# ==================== 2. å‚æ•°é…ç½® ====================

# [è·¯å¾„è®¾ç½®]
# æ¨¡å‹è·¯å¾„ (ä¿æŒä½ åŸæœ¬çš„ç»å¯¹è·¯å¾„ï¼Œæˆ–è€…ç§»å…¥é¡¹ç›®å†…ç”¨ $PROJECT_ROOT/models/...)
MODEL_PATH="/root/autodl-tmp/subspace/models/LLM-Research/Meta-Llama-3-8B-Instruct"

# æ•°æ®è·¯å¾„ (ä¿®æ”¹ä¸ºæŒ‡å‘æˆ‘ä»¬æ–°å»ºçš„ data ç›®å½•)
DATA_PATH="$PROJECT_ROOT/data/commonsense170k/train.json"

# è¾“å‡ºè·¯å¾„ (ç»Ÿä¸€æ”¾å…¥ outputs æ–‡ä»¶å¤¹ï¼Œæ–¹ä¾¿ç®¡ç†)
OUTPUT_DIR="$PROJECT_ROOT/outputs/fira"

# [æ˜¾å¡è®¾ç½®]
export CUDA_VISIBLE_DEVICES=0

# ==================== 3. å¯åŠ¨è®­ç»ƒ ====================
echo "ğŸš€ å¼€å§‹è¿è¡Œ Fira è®­ç»ƒ..."
echo "ğŸ“‚ æ•°æ®è·¯å¾„: $DATA_PATH"
echo "ğŸ’¾ è¾“å‡ºè·¯å¾„: $OUTPUT_DIR"

# æ³¨æ„ï¼šè¿™é‡Œçš„ python è„šæœ¬è·¯å¾„æ˜¯ç›¸å¯¹äº subspace/ çš„
python Fira/fine_tuning/finetune.py \
  --base_model "$MODEL_PATH" \
  --data_path "$DATA_PATH" \
  --output_dir "$OUTPUT_DIR" \
  --batch_size 64 \
  --micro_batch_size 8 \
  --num_epochs 3 \
  --learning_rate 2e-4 \
  --cutoff_len 1024 \
  --val_set_size 1000 \
  --adapter_name lora \
  --lora_r 16 \
  --lora_alpha 32 \
  --target_modules '["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]' \
  --save_step 1000 \
  --eval_step 50 \
  --optimizer_name fira_adamw

echo "âœ… è®­ç»ƒç»“æŸï¼"