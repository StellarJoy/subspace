import os
import subprocess
import sys

# ==================== 1. é­”æ³•å‰ç¼€ (è‡ªåŠ¨å®šä½å·¥ç¨‹ç›®å½•) ====================
# è·å–å½“å‰è„šæœ¬ä½ç½® (subspace/scripts/batch_eval.py)
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
# è·å–é¡¹ç›®æ ¹ç›®å½• (subspace/)
PROJECT_ROOT = os.path.dirname(SCRIPT_DIR)

print(f"ğŸ“ é¡¹ç›®æ ¹ç›®å½•å·²å®šä½: {PROJECT_ROOT}")

# ==================== 2. æ ¸å¿ƒé…ç½®åŒº ====================

# [åŸºåº§æ¨¡å‹è·¯å¾„]
# ä½¿ç”¨ os.path.join æ‹¼æ¥ï¼Œå…¼å®¹å„ç§æ“ä½œç³»ç»Ÿï¼Œä¸”ä¸ä¾èµ–ç»å¯¹è·¯å¾„
BASE_MODEL_PATH = os.path.join(PROJECT_ROOT, "models/LLM-Research/Meta-Llama-3-8B-Instruct")

# [æµ‹è¯•ä»»åŠ¡]
TASKS = "piqa,boolq,arc_easy"

# [å¾…æµ‹æ¨¡å‹åˆ—è¡¨]
# æ ¼å¼: "æ˜¾ç¤ºåç§°": "ç›¸å¯¹äº outputs çš„æ–‡ä»¶å¤¹å" (æˆ–è€…å®Œæ•´è·¯å¾„)
# æˆ‘æ ¹æ®ä¹‹å‰å¸®ä½ æ”¹çš„è„šæœ¬ï¼Œå¡«å¥½äº†è¿™ä¸‰ä¸ªè·¯å¾„ï¼Œä½ å¯ä»¥æ ¹æ®å®é™…è·‘å‡ºæ¥çš„æƒ…å†µè°ƒæ•´
MODELS_TO_TEST = {
    # LLaMA-Factory çš„ç»“æœ (ä¹‹å‰æ”¹åä¸º llama3-8b-dora-sft)
    "LoRA_Factory": os.path.join(PROJECT_ROOT, "outputs/lora_rank8"),

    # Fira çš„ç»“æœ (ä¹‹å‰è„šæœ¬é‡Œè®¾å®šçš„)
    #"Fira_LoRA":    os.path.join(PROJECT_ROOT, "outputs/fira_llama3_8b"),

    # Stella çš„ç»“æœ (ä¹‹å‰è„šæœ¬é‡Œè®¾å®šçš„)
    #"Stella_H800":  os.path.join(PROJECT_ROOT, "outputs/llama3_stella_h800_speed_run"),
}

# [è¯„ä¼°ç»“æœä¿å­˜ä½ç½®]
# å»ºè®®ä¹Ÿæ”¾åœ¨ outputs é‡Œï¼Œæˆ–è€…å•ç‹¬ä¸€ä¸ª eval_results æ–‡ä»¶å¤¹
EVAL_OUTPUT_DIR = os.path.join(PROJECT_ROOT, "eval_results")

# [æ˜¾å¡è®¾ç½®]
GPU_ID = "0"

# ====================================================

def run_eval():
    print(f"\nğŸš€ å¼€å§‹æ‰¹é‡è¯„ä¼°ä»»åŠ¡: {TASKS}")
    print(f"ğŸ“‚ ç»“æœå°†ä¿å­˜åœ¨: {EVAL_OUTPUT_DIR}\n")

    # ç¡®ä¿ç»“æœç›®å½•å­˜åœ¨
    os.makedirs(EVAL_OUTPUT_DIR, exist_ok=True)

    # è®¾ç½®ç¯å¢ƒå˜é‡ (å¦‚æœéœ€è¦)
    os.environ["HF_TOKEN"] = "" # å¦‚æœæœåŠ¡å™¨å·²æœ‰ç¯å¢ƒæ— éœ€é‡å¤è®¾ç½®
    os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"

    for algo_name, model_path in MODELS_TO_TEST.items():
        print(f"{'='*60}")
        print(f"ğŸ§ª æ­£åœ¨è¯„ä¼°: {algo_name}")
        print(f"ğŸ” æ¨¡å‹è·¯å¾„: {model_path}")
        
        # æ£€æŸ¥è·¯å¾„æ˜¯å¦å­˜åœ¨
        if not os.path.exists(model_path):
            print(f"âŒ è­¦å‘Š: è·¯å¾„ä¸å­˜åœ¨ï¼Œè·³è¿‡æ­¤æ¨¡å‹ï¼")
            continue

        # --- æ™ºèƒ½åˆ¤æ–­æ¨¡å‹ç±»å‹ ---
        # æ£€æŸ¥ç›®å½•ä¸‹æ˜¯å¦æœ‰ adapter_config.json
        is_adapter = os.path.exists(os.path.join(model_path, "adapter_config.json"))
        
        if is_adapter:
            print("ğŸ¤– ç±»å‹è¯†åˆ«: PEFT Adapter (LoRA/DoRA/PiSSA)")
            # è¯­æ³•: pretrained=BaseModel,peft=AdapterPath
            model_args = f"pretrained={BASE_MODEL_PATH},peft={model_path},dtype=float16"
        else:
            print("ğŸ¤– ç±»å‹è¯†åˆ«: Full Model (å…¨é‡æƒé‡)")
            # è¯­æ³•: pretrained=ModelPath
            model_args = f"pretrained={model_path},dtype=float16"

        # æ„é€ è¾“å‡ºæ–‡ä»¶è·¯å¾„
        output_file = os.path.join(EVAL_OUTPUT_DIR, f"result_{algo_name}.json")
        
        # æ„é€  lm_eval å‘½ä»¤
        cmd = [
            "lm_eval",
            "--model", "hf",
            "--model_args", model_args,
            "--tasks", TASKS,
            "--num_fewshot", "0",
            "--batch_size", "auto",
            "--device", f"cuda:{GPU_ID}",
            "--output_path", output_file
        ]

        # æ‰“å°å¹¶æ‰§è¡Œå‘½ä»¤
        print(f"ğŸƒ æ‰§è¡Œå‘½ä»¤: {' '.join(cmd)}")
        
        try:
            # å®æ—¶è¾“å‡ºæ—¥å¿—
            subprocess.run(cmd, check=True)
            print(f"âœ… {algo_name} è¯„ä¼°å®Œæˆï¼")
        except subprocess.CalledProcessError:
            print(f"âŒ {algo_name} è¯„ä¼°è¿‡ç¨‹ä¸­å‡ºé”™ï¼")
        except FileNotFoundError:
            print("âŒ é”™è¯¯: æœªæ‰¾åˆ° 'lm_eval' å‘½ä»¤ã€‚è¯·ç¡®ä¿ä½ å·²ç» pip install lm-eval å¹¶ä¸”ç¯å¢ƒå·²æ¿€æ´»ã€‚")

    print(f"\nğŸ‰ æ‰€æœ‰æµ‹è¯•ç»“æŸï¼è¯·æŸ¥çœ‹ {EVAL_OUTPUT_DIR} æ–‡ä»¶å¤¹ã€‚")

if __name__ == "__main__":
    run_eval()